1. Для чего и в каких случаях полезны различные варианты усреднения для метрик качества классификации: micro, macro, weighted?

Macro avg  - это простое среднее precision и recall по каждому из классов.
Micro avg  - это гармоническое среднее precision и recall.
Weighted avg - это взвешенное среднее по каждому из классов.

Если разница между Macro avg и Micro avg величинами довольно большая, то можно сказать, что несбалансированность 
классов вносит существенный вклад в значение целевой метрики. 
Если Weighted avg на test существенно отличается от распределения классов на train, то можно говорить о том, 
что модель довольно плохо определяет принадлежность объекта к классу.


2. В чём разница между моделями xgboost, lightgbm и catboost или какие их основные особенности?

По дефолту Xgboost и catboost обучаются на всей выборке, light_gbm - только на части.
light_gbm и catboost не требуют максимально точной настройки гиперпараметров,  Xgboost требует.
Во всех реализациях градиентного бустинга очень важными гиперпараметрами в light_gbm являются глубина дерева и минимальное количество признаков в листе.
Во вторую очередь необходимо настраивать скорость обучения и количество итераций, а в конце - количество листьев.
Однако, если данных довольно много, то количество листьев можно настраивать в первую очередь.
Xgboost можно обучать на половине выборке, оптимальное значение количества выборки для light_gbm - около одной трети.
Для light_gbm крайне полезно использовать большее количество признаков. Это делает модель не только более точной, но и позволяет 
получить более устойчивые оценки.
Также реализации отличаются базовым алгоритмом.
В Xgboost это pre-sorted algorithm либо Histogram-based algorithm, в light_gbm - One-Side Sampling.
В light_gbm на каждой итерайии алгоритм выбирает около 10 процентов максимальной величины тангенса наклона функции потерь,
остальные выбираются случайным образом. Это решение основано на предположении о том, что величины с небольшим градиентом
имеют меньшую ошибку обучения и уже хорошо обучены. В принципе, это предположение не лишено логики, поскольку на начальном 
этапе, не зная оптимального вектора градиента, довольно опрометчиво полагаться на первые попавшиеся хорошие значения градиента.
Таким образом, light_gbm обеспечивает хороший баланс между глубиной дерева и количеством данных, используемом при обучении.
В Xgboost базовый алгоритм разбивает выборку на дискретные величины и обучение происходит на каждой этой величине.
Это и определяет то, что, как правило, оценки Xgboost являются более точными, но и наиболее затратными в плане вычислений
и времени прототипирования.
В light_gbm и cat_boost при вызове объекта есть возможность обозначить категориальные параметры, в 
Xgboost необходимо заранее обозначить категориальные переменные в датасете, например, используя one hot encoding.
В целом, light_gbm из всех трёх реализаций мне кажется наиболее удачным и широко применимым, поскольку обеспечивает
высокую скорость вычислений и довольно небольшие требования к тонкости настройки гиперпараметров(высокая скорость прототипирования).
